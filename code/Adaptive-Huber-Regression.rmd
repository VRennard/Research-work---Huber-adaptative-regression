---
title: "A Real Data Example :  House Price Prediction"
author: "MADALI Nabil,Virgile Rennard"
date: "Decembre 31, 2019"
output: 
  html_document:
    keep_md: true
---
```{r setup, include=FALSE,message=FALSE,error=FALSE,warning=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	error=FALSE,
	message = FALSE,
	warning = FALSE,
	##cache = TRUE,
	tidy = TRUE,
	tidy.opts = list(width.cutoff = 70)
)
```
# Introduction
Founded in 2010, Kaggle is a Data Science platform where users can share, collaborate, and compete. One key feature of Kaggle is “Competitions”, which offers users the ability to practice on real-world data and to test their skills with, and against, an international community.

We’ll work through the House Prices: Advanced Regression Techniques competition.The challenge is to predict the final sale price of the homes. This information is stored in the SalePrice column.

# Data
The House Prices: Advanced Regression Techniques is a competition from Kaggle (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/ ). The full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here .Here's a brief version of what you'll find in the data description file.

**SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict.

**MSSubClass**: The building class

**LandSlope**: Slope of property

**Neighborhood**: Physical locations within Ames city limits
 present)
 
**BldgType**: Type of dwelling

**HouseStyle**: Style of dwelling

**OverallQual**: Overall material and finish quality

**OverallCond**: Overall condition rating

**YearBuilt**: Original construction date

**YearRemodAdd**: Remodel date

# Loading and data pre-processing

## Load data

```{r}
train=read.csv("train.csv",stringsAsFactors = FALSE)
test=read.csv("test.csv",stringsAsFactors = FALSE)

## Let us look at the dimensions of these datasets and structure of train 
dim(train)
dim(test)
str(train)
```

## Data pre-processing

```{r}
## Save the ID column so that we can drop it from merged dataset 
train_ID=train$Id
test_ID=test$Id

## test doesn't have SalePrice column, so add it.
test$SalePrice=NA
```

**Removing outliers** - A scatterplot between SalePrice and GrLivArea shows a couple of outliers. Let us get rid of them.

```{r}
library(ggplot2)
qplot(train$GrLivArea,train$SalePrice,main="With Outliers")

```

**Log Transformation of SalePrice Variable** - In order to make the distribution of the target variable normal, we need to transform it by taking log.

```{r}
## Plot histogram of SalePrice Variable - Right skewed
qplot(SalePrice,data=train,bins=50,main="Right skewed distribution")

## Log transformation of the target variable
train$SalePrice <- log(train$SalePrice + 1)

## Normal distribution after transformation
qplot(SalePrice,data=train,bins=50,main="Normal distribution after log transformation")
```

**Combine train and test datasets**.

```{r}
## Combine train and test
combi=rbind(train,test)

## Dropping Id as it is unnecessary for the prediction process.
combi=combi[,-1]
```

# Data Processing and Analysis

## Checking Missing data

Let us check the number of rows of data missing for each variable out of 2917 rows.

```{r}
colSums(is.na(combi))
```

Clearly, there are a lot of missing values. PoolQC, MiscFeature, Alley and Fence have 90% of the data as NA. 

## Imputing Missing data

We will be handling each variable separately. 

1. For most of the **categorical features**, NA values will be imputed as **'None'**, because referring to the **data_description.txt** file, **the NA of these variables represent values such as 'No Garage','No Basement', etc.**

2. For most of the **numerical features**, NA values will be replaced by 0, for variables like GarageArea, GarageCars, etc.

3. For some categorical features like Functional and Electrical, the NA values will be replaced by the most frequently occuring value for that variable.

```{r}
## For some variables, fill NA with "None" 
for(x in c("Alley","PoolQC","MiscFeature","Fence","FireplaceQu","GarageType","GarageFinish","GarageQual",'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',"MasVnrType")){
        combi[is.na(combi[,x]),x]="None"
}

#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood
temp=aggregate(LotFrontage~Neighborhood,data=combi,median)
temp2=c()
for(str in combi$Neighborhood[is.na(combi$LotFrontage)]){temp2=c(temp2,which(temp$Neighborhood==str))}
combi$LotFrontage[is.na(combi$LotFrontage)]=temp[temp2,2]

## Replacing missing data with 0
for(col in c('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',"MasVnrArea")){
        combi[is.na(combi[,col]),col]=0
}

## Replace missing MSZoning values by "RL"
combi$MSZoning[is.na(combi$MSZoning)]="RL"

## Remove Utilities as it has zero variance
combi=combi[,-9]

## Replace missing Functional values with "Typ"
combi$Functional[is.na(combi$Functional)]="Typ"

## Replace missing Electrical values with "SBrkr"
combi$Electrical[is.na(combi$Electrical)]="SBrkr"

## Replace missing KitchenQual values by "TA"
combi$KitchenQual[is.na(combi$KitchenQual)]="TA"

## Replace missing SaleType values by "WD"
combi$SaleType[is.na(combi$SaleType)]="WD"

## Replace missing Exterior1st and Exterior2nd values by "VinylSd"
combi$Exterior1st[is.na(combi$Exterior1st)]="VinylSd"
combi$Exterior2nd[is.na(combi$Exterior2nd)]="VinylSd"

## All NAs should be gone, except the test portion of SalePrice variable, which we ourselves had initialized to NA earlier.
colSums(is.na(combi))
```

## Transforming some numerical variables that are really categorical

```{r}
combi$MSSubClass=as.character(combi$MSSubClass)
combi$OverallCond=as.character(combi$OverallCond)
combi$YrSold=as.character(combi$YrSold)
combi$MoSold=as.character(combi$MoSold)
```

## Label Encoding some categorical variables that may contain information in their ordering set

**We will also specify the order of the levels (mapping), while label encoding (converting categories to integer ranks - 1 to n) the categorical variables.**

```{r}
cols = c('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope','LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold')

FireplaceQu=c('None','Po','Fa','TA','Gd','Ex')
BsmtQual=c('None','Po','Fa','TA','Gd','Ex')
BsmtCond=c('None','Po','Fa','TA','Gd','Ex')
GarageQual=c('None','Po','Fa','TA','Gd','Ex')
GarageCond=c('None','Po','Fa','TA','Gd','Ex')
ExterQual=c('Po','Fa','TA','Gd','Ex')
ExterCond=c('Po','Fa','TA','Gd','Ex')
HeatingQC=c('Po','Fa','TA','Gd','Ex')
PoolQC=c('None','Fa','TA','Gd','Ex')
KitchenQual=c('Po','Fa','TA','Gd','Ex')
BsmtFinType1=c('None','Unf','LwQ','Rec','BLQ','ALQ','GLQ')
BsmtFinType2=c('None','Unf','LwQ','Rec','BLQ','ALQ','GLQ')
Functional=c('Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ')
Fence=c('None','MnWw','GdWo','MnPrv','GdPrv')
BsmtExposure=c('None','No','Mn','Av','Gd')
GarageFinish=c('None','Unf','RFn','Fin')
LandSlope=c('Sev','Mod','Gtl')
LotShape=c('IR3','IR2','IR1','Reg')
PavedDrive=c('N','P','Y')
Street=c('Pave','Grvl')
Alley=c('None','Pave','Grvl')
MSSubClass=c('20','30','40','45','50','60','70','75','80','85','90','120','150','160','180','190')
OverallCond=NA
MoSold=NA
YrSold=NA
CentralAir=NA
levels=list(FireplaceQu, BsmtQual, BsmtCond, GarageQual, GarageCond, ExterQual, ExterCond,HeatingQC, PoolQC, KitchenQual, BsmtFinType1, BsmtFinType2, Functional, Fence, BsmtExposure, GarageFinish, LandSlope,LotShape, PavedDrive, Street, Alley, CentralAir, MSSubClass, OverallCond, YrSold, MoSold)
i=1
for (c in cols){
        if(c=='CentralAir'|c=='OverallCond'|c=='YrSold'|c=='MoSold'){
                combi[,c]=as.numeric(factor(combi[,c]))}
        else
                combi[,c]=as.numeric(factor(combi[,c],levels=levels[[i]]))
i=i+1
        }
```

## Adding an important feature - Total area of basement

```{r}
combi$TotalSF=combi$TotalBsmtSF+combi$X1stFlrSF+combi$X2ndFlrSF
```

## Getting dummy categorical features

```{r}
# first get data type for each feature
feature_classes <- sapply(names(combi),function(x){class(combi[[x]])})
numeric_feats <-names(feature_classes[feature_classes != "character"])

# get names of categorical features
categorical_feats <- names(feature_classes[feature_classes == "character"])

# use caret dummyVars function for hot one encoding for categorical features
library(caret)
dummies <- dummyVars(~.,combi[categorical_feats])
categorical_1_hot <- predict(dummies,combi[categorical_feats])
```

## Fixing Skewed features

**We will transform the skewed features with BoxCox Transformation.**

```{r}
## Determine skew for each numeric feature
library(moments)
library(MASS)
skewed_feats <- sapply(numeric_feats,function(x){skewness(combi[[x]],na.rm=TRUE)})

## Keep only features that exceed a threshold (0.75) for skewness
skewed_feats <- skewed_feats[abs(skewed_feats) > 0.75]

## Transform skewed features with boxcox transformation
for(x in names(skewed_feats)) {
  bc=BoxCoxTrans(combi[[x]],lambda = .15)
  combi[[x]]=predict(bc,combi[[x]])
  #combi[[x]] <- log(combi[[x]] + 1)
}
```

## Reconstruct all data with pre-processed data.

```{r}
combi <- cbind(combi[numeric_feats],categorical_1_hot)

## Let us look at the dimensions of combi.
dim(combi)
```

# Model building and evaluation

## Splitting train dataset further into Training and Validation in order to evaluate the models

```{r}
training<-combi[1:1458,]
testing<-combi[1459:2917,]
set.seed(222)
inTrain<-createDataPartition(y=training$SalePrice,p=.7,list=FALSE)
Training<-training[inTrain,]
Validation<-training[-inTrain,]
```

## Models
To measure the predictive performance, we consider a robust prediction loss:  
the mean absolute error (MAE) defined as :
        
\begin{equation*}
           \frac{1}{n_{test}} = \sum_i^{n_{test}} \mid y_i^{test} - <x_i^{test},\hat{\beta}> \mid
\end{equation*}

 where $y_i^{test}$ and $x_i^{test}$,i=1,...,$n_{test}$ denote  the  observations  of  the  response  and predictor variables in the test data, respectively.
 
## Lasso - Regularized Regression

**Build model, predict SalePrice for Validation set and evaluate the RMSE score.**

```{r}
library(glmnet)
library(Metrics)
set.seed(123)
cv_lasso=cv.glmnet(as.matrix(Training[,-59]),Training[,59])

## Predictions
preds<-predict(cv_lasso,newx=as.matrix(Validation[,-59]),s="lambda.min")
rmse(Validation$SalePrice,preds)
```
# Adaptive Huber Regression

```{r}
library(tfHuber)

listHuber = huberReg(as.matrix(Training[,-59]),Training[,59])
thetaHuber = listHuber$theta

X=Validation[,-59]

pred=0
for (i in 1:435){
  pred=c(pred,c(1,unname(unlist(X[i,])))  %*% thetaHuber)
}
pred=pred[2:436]


rmse(Validation$SalePrice,pred)

```

We choose robustification and regularization parameters as follows:
\begin{equation}
            
      \tau=c_\tau \times \hat \sigma (\frac{n_{eff}}{t})^{1/2} \qquad \text{and  } \lambda=c_\lambda \times \hat \sigma (\frac{n_{eff}}{t})^{1/2}
\end{equation}
where  $\hat \sigma^2=\frac{1}{n} \sum_{i=1}^{n}(y_i - \bar{y})^2$  with  $\bar{y}  = \frac{1}{n} \sum_{i=1}^{n}$ y_i serves  as  a  crude  preliminary estimate of $\sigma^2$ , and the parameter t controls the confidence level.  We set t= logn for simplicity except for the phase transition plot
        
        
```{r}
library(tfHuber)

n=1458
d=220
Y = Training[,59]

n_eff=n/log(d)

tau=0.2 *sd(Y)*((n_eff/log(n))^(0.5))

listHuber = huberReg(as.matrix(Training[,-59]),Training[,59],constTau=tau)
thetaHuber = listHuber$theta

X=Validation[,-59]

pred=0
for (i in 1:435){
  pred=c(pred,c(1,unname(unlist(X[i,])))  %*% thetaHuber)
}
pred=pred[2:436]
rmse(Validation$SalePrice,pred)
```

# Simple Linear Regression
```{r}
fit0 <- lm(Training[,59]~ as.matrix(Training[,-59])) 
coeff=unname(fit0$coefficients)[2:length(fit0$coefficients)]
coeff[is.na(coeff)] <- 0
b=unname(fit0$coefficients)[1]

preds=as.matrix(Validation[,-59]) %*% coeff + rep(1,435)*b

rmse(Validation$SalePrice,preds)

```


Lasso clearly shows the smallest MAE, followed by AHuber and Linear Model.  The Lasso produces a fairly large model despite the small sample.Now it has been recognized that Lasso tends to select many noise variables along with the significant ones, especially when data exhibit heavy tails.

Due to the singularity of the input matrix  we can not use the standard Huber Regresion,in our experiment, we have seen that if we have a singulare input matrix, the adaptive Huber regression will have the same performance as the  standard regression algorithms, but if we condition the fact that the input  must be  nonsingular, the Adabtive  huber regression will surpass the standard regression algorithm.